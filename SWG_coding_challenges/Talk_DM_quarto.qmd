---
title: "Coding Stochastic Weather Generators: Challenges and Perspectives"
author: "David Métivier"
date: last-modified
format:
    html:
        toc: true
engine: julia
julia:
    exeflags: ["--threads=auto"]
---

The notebook is the companion to the talk [Coding Stochastic Weather Generators: Challenges and Perspectives](https://swgen2025.sciencesconf.org/resource/page/id/21) given at the [SWGEN 2025 - Conference on Stochastic Weather Generators](https://swgen2025.sciencesconf.org/){preview-link="true"}.
<iframe src="https://swgen2025.sciencesconf.org/" style="
                width: 100%;
            ">
</iframe>

It showcases two challenges encountered when developing Stochastic Weather Generators (SWGs):

- Simulating SWGs efficiently, through a benchmark of HMM simulation in Julia, R, (Python) and C. It uses LLMs to generate the code in each language to mimic what a typical student/scientist might write.
- Fitting SWGs. It uses a Matérn Gaussian Random Field (GRF) as a running example to illustrate parameter estimation via gradient-based optimization with automatic differentiation, finite difference and gradient free methods e.g. Nelder-Mead.

```{julia}
cd(@__DIR__)
import Pkg
Pkg.activate(".");
using PrettyTables
```

```{julia}
# import PythonCall #if you want to test Python version
import RCall
```

# Benchmarking HMM Simulation across languages

All implementations have been generated by a LLM. All versions could be further optimized, but the goal here is to illustrate typical performance differences "out of the box".
We will illustrate with a simple Hidden Markov Model (HMM) with 4 states and 12-dimensional multivariate normal emissions.

## Julia Version

In the Julia version, we use `Distributions.jl` for an arbitrary distribution interface. 
Thanks to Julia's multiple dispatch, switching to other emission distributions (e.g., Poisson, Gamma) is straightforward.

```{julia}
using Distributions, LinearAlgebra

function rand_HMM(Q, dist, N)
    Z = zeros(Int, N)
    d = length(dist[1])  # dimension of observations
    Y = zeros(d, N)
    Z[1] = 1
    Y[:, 1] = rand(dist[Z[1]])
    for t in 2:N
        Z[t] = rand(Categorical(Q[Z[t-1], :]))
        Y[:, t] = rand(dist[Z[t]])
    end
    return Z, Y
end
```

## R Version 

In the R version, we use the `mvtnorm` package for multivariate normal sampling. 
Compared to the Julia version above, the MvNormal distribution has to be hard-coded.

```{r}
library(mvtnorm)

rand_HMM <- function(Q, means_list, Sigma_list, N) {
    n_states <- nrow(Q)
    d <- length(means_list[[1]])  # dimension
    Z <- integer(N)
    Y <- matrix(0, nrow = d, ncol = N)
    
    Z[1] <- 1
    Y[, 1] <- rmvnorm(1, mean = means_list[[Z[1]]], sigma = Sigma_list[[Z[1]]])
    
    for (t in 2:N) {
        Z[t] <- sample(1:n_states, size = 1, prob = Q[Z[t-1], ])
        Y[, t] <- rmvnorm(1, mean = means_list[[Z[t]]], sigma = Sigma_list[[Z[t]]])
    }
    return(list(Z = Z, Y = Y))
};
```

## Python Version 

The Python version also has to hard-code the MvNormal distribution. It was not tested because Python is not installed on my computer.

```{python}
#| eval: false
import numpy as np

def rand_HMM(Q, means_list, Sigma_list, N):
    n_states = len(Q)
    d = len(means_list[0])  # dimension
    Z = np.zeros(N, dtype=int)
    Y = np.zeros((d, N))
    
    Z[0] = 0
    Y[:, 0] = np.random.multivariate_normal(means_list[Z[0]], Sigma_list[Z[0]])
    
    for t in range(1, N):
        Z[t] = np.random.choice(n_states, p=Q[Z[t-1], :])
        Y[:, t] = np.random.multivariate_normal(means_list[Z[t]], Sigma_list[Z[t]])
    
    return Z, Y
```

## C Version (called from Julia)

Here is the C code generated by the LLM on the first try. 
It performs manual Cholesky decomposition and Box-Muller sampling of standard normals.

```{julia}
using Libdl # Dynamic Linker to find path for C
```

**The C code is folded below**.
```{julia}
#| code-fold: true
C_code_v1 = """
#include <stddef.h>
#include <stdlib.h>
#include <math.h>
#include <string.h>

// Cholesky decomposition (simple version, assumes positive definite)
void cholesky(double *A, double *L, size_t d) {
    memset(L, 0, d * d * sizeof(double));
    for (size_t i = 0; i < d; i++) {
        for (size_t j = 0; j <= i; j++) {
            double sum = 0.0;
            for (size_t k = 0; k < j; k++) {
                sum += L[i * d + k] * L[j * d + k];
            }
            if (i == j) {
                L[i * d + j] = sqrt(A[i * d + i] - sum);
            } else {
                L[i * d + j] = (A[i * d + j] - sum) / L[j * d + j];
            }
        }
    }
}

void rand_HMM_c_v1(size_t N, size_t d, double *Q, size_t n_states, 
                double *means, double *Sigmas, 
                int *Z, double *Y, unsigned int seed) {
    srand(seed);
    
    // Precompute Cholesky decompositions
    double *L_all = (double *)malloc(n_states * d * d * sizeof(double));
    for (size_t s = 0; s < n_states; s++) {
        cholesky(&Sigmas[s * d * d], &L_all[s * d * d], d);
    }
    
    Z[0] = 0;
    // Sample first observation
    for (size_t j = 0; j < d; j++) {
        double z = 0.0;
        for (size_t k = 0; k < d; k++) {
            double u1 = (double)rand() / RAND_MAX;
            double u2 = (double)rand() / RAND_MAX;
            double std_normal = sqrt(-2.0 * log(u1)) * cos(2.0 * M_PI * u2);
            z += L_all[Z[0] * d * d + j * d + k] * std_normal;
        }
        Y[j * N + 0] = means[Z[0] * d + j] + z;
    }
    
    for (size_t t = 1; t < N; t++) {
        // Sample next state
        double r = (double)rand() / RAND_MAX;
        double cum_prob = 0.0;
        for (size_t s = 0; s < n_states; s++) {
            cum_prob += Q[Z[t-1] * n_states + s];
            if (r <= cum_prob) {
                Z[t] = s;
                break;
            }
        }
        
        // Sample multivariate normal observation
        for (size_t j = 0; j < d; j++) {
            double z = 0.0;
            for (size_t k = 0; k < d; k++) {
                double u1 = (double)rand() / RAND_MAX;
                double u2 = (double)rand() / RAND_MAX;
                double std_normal = sqrt(-2.0 * log(u1)) * cos(2.0 * M_PI * u2);
                z += L_all[Z[t] * d * d + j * d + k] * std_normal;
            }
            Y[j * N + t] = means[Z[t] * d + j] + z;
        }
    }
    
    free(L_all);
}
"""
```

The Julia wrapper to compile and call the C function:
```{julia}
const Clib_path_v1 = tempname()

open(`gcc -fPIC -O3 -ffast-math -msse3 -xc -shared -o $(Clib_path_v1 * "." * Libdl.dlext) -`, "w") do f
    print(f, C_code_v1)
end

function c_rand_HMM_v1(Q::Matrix{Float64}, means::Vector{Vector{Float64}},
    Sigmas::Vector{Matrix{Float64}}, N::Int)
    n_states = size(Q, 1)
    d = length(means[1])
    Z = zeros(Int32, N)
    Y = zeros(Float64, d, N)

    # Flatten means and Sigmas for C
    means_flat = reduce(vcat, means)
    Sigmas_flat = reduce(vcat, [vec(Sigmas[s]') for s in 1:n_states])

    seed = rand(UInt32)
    ccall(("rand_HMM_c_v1", Clib_path_v1), Cvoid,
        (Csize_t, Csize_t, Ptr{Float64}, Csize_t, Ptr{Float64}, Ptr{Float64},
            Ptr{Int32}, Ptr{Float64}, UInt32),
        N, d, Q, n_states, means_flat, Sigmas_flat, Z, Y, seed)
    return Z, Y
end
```

Here is another version assuming that the Cholesky factors of the covariance matrices are precomputed in Julia and passed to C, along with standard normal random variables.

**The C code is folded below**.
```{julia}
#| code-fold: true
C_code_v2 = """
#include <stddef.h>
#include <stdlib.h>
#include <string.h>

// rand_HMM_c: assumes Julia provides L_all (Cholesky factors) and E (standard normals)
// Q: n_states x n_states row-stochastic transition matrix
// means: n_states x d means (flattened row-major per state)
// L_all: n_states x d x d lower-triangular Cholesky factors (flattened)
// E: d x N matrix of standard normals (flattened column-major as passed from Julia)
void rand_HMM_c_v2(size_t N, size_t d, double *Q, size_t n_states,
                double *means, double *L_all, double *E,
                int *Z, double *Y, unsigned int seed) {
    // Simple LCG for reproducible state sampling (optional: srand)
    srand(seed);

    // Initial state
    Z[0] = 0;

    // First observation: y = μ + L * e
    for (size_t j = 0; j < d; j++) {
        double z = 0.0;
        for (size_t k = 0; k < d; k++) {
            double e = E[k * N + 0];
            z += L_all[Z[0] * d * d + j * d + k] * e;
        }
        Y[j * N + 0] = means[Z[0] * d + j] + z;
    }

    for (size_t t = 1; t < N; t++) {
        // Sample next state from Q[Z[t-1], :]
        double r = (double)rand() / RAND_MAX;
        double cum_prob = 0.0;
        for (size_t s = 0; s < n_states; s++) {
            cum_prob += Q[Z[t-1] * n_states + s];
            if (r <= cum_prob) {
                Z[t] = s;
                break;
            }
        }

        // Observation: y = μ + L * e
        for (size_t j = 0; j < d; j++) {
            double z = 0.0;
            for (size_t k = 0; k < d; k++) {
                double e = E[k * N + t];
                z += L_all[Z[t] * d * d + j * d + k] * e;
            }
            Y[j * N + t] = means[Z[t] * d + j] + z;
        }
    }
}
"""
```

```{julia}
const Clib_path_v2 = tempname()

open(`gcc -fPIC -O3 -ffast-math -msse3 -xc -shared -o $(Clib_path_v2 * "." * Libdl.dlext) -`, "w") do f
    print(f, C_code_v2)
end

function c_rand_HMM_v2(Q::Matrix{Float64}, means::Vector{Vector{Float64}},
    Sigmas::Vector{Matrix{Float64}}, N::Int)
    n_states = size(Q, 1)
    d = length(means[1])
    Z = zeros(Int32, N)
    Y = zeros(Float64, d, N)
    # Flatten means for C
    means_flat = reduce(vcat, means)
    # Precompute Cholesky factors in Julia and flatten for C
    L_all = reduce(vcat, [vec(cholesky(Sigmas[s]).L') for s in 1:n_states])
    # Generate standard normals in Julia (d x N) and flatten column-major
    E = randn(d, N)

    seed = rand(UInt32)
    ccall(("rand_HMM_c_v2", Clib_path_v2), Cvoid,
        (Csize_t, Csize_t, Ptr{Float64}, Csize_t, Ptr{Float64}, Ptr{Float64}, Ptr{Float64},
            Ptr{Int32}, Ptr{Float64}, UInt32),
        N, d, Q, n_states, means_flat, L_all, E, Z, Y, seed)
    return Z, Y
end
```

## Benchmark Setup

Define a 4-state HMM with 12-dimensional multivariate normal emissions:

```{julia}
using BenchmarkTools
using Random

Random.seed!(123)

# Transition matrix
Q = [0.7 0.15 0.10 0.05;
    0.2 0.6 0.15 0.05;
    0.1 0.2 0.6 0.10;
    0.05 0.1 0.15 0.7]

# Generate parameters for 4 states, 12 dimensions
d = 12
n_states = 4
means_vec = [randn(d) .+ i for i in 1:n_states]
Sigmas_vec = [Matrix(Diagonal(rand(d) .+ 0.5)) for _ in 1:n_states]

# Julia version with Distributions.jl
dist = [MvNormal(means_vec[i], Sigmas_vec[i]) for i in 1:n_states]

N = 10_000  # 10k time steps ≃ 27 years of daily data
```

### Julia Benchmark

```{julia}
time_julia = @belapsed rand_HMM(Q, dist, N);
```

### Pass parameters to R

```{julia}
RCall.@rput Q
RCall.@rput N
RCall.@rput d
RCall.@rput n_states
RCall.@rput means_vec
RCall.@rput Sigmas_vec;
```

### R Benchmark

```{julia}
time_R = @belapsed RCall.R"rand_HMM(Q, means_vec, Sigmas_vec, N)";
```

### C Benchmark

```{julia}
time_C_v1 = @belapsed c_rand_HMM_v1(Q, means_vec, Sigmas_vec, N);
```

```{julia}
time_C_v2 = @belapsed c_rand_HMM_v2(Q, means_vec, Sigmas_vec, N);
```

## Results Summary

```{julia}
#| code-fold: true
results_table = [
    "Julia (baseline)" time_julia 1.0;
    "C v1" time_C_v1 time_C_v1/time_julia;
    "C v2" time_C_v2 time_C_v2/time_julia;
    "R" time_R time_R/time_julia
]

pretty_table(results_table,
    column_labels=["Language", "Time (s)", "Relative Speed"], backend=:html)
```

## Key Takeaways

1. **R loops are unavoidably slow**: R is ~600× slower than Julia for this loop-heavy simulation. While vectorization helps for some operations, SWG simulations require sequential time-stepping that cannot be vectorized. Optimizing R code is limited. Typically one would try to parallelize with `mclapply` or rewrite bottlenecks in C/C++ via Rcpp, but this adds complexity and maintenance burden.

2. **C is not always faster**: Surprisingly, the original C implementation by the LLM is ~50× slower than Julia! This demonstrates that:
   - Writing efficient C requires deep expertise (memory management, compiler flags, algorithm choices)
   - Bad C code can easily be slower than high-level languages with good JIT compilers
   - The complexity of C makes it unsuitable for rapid prototyping in scientific applications

3. **Julia solves the two-language problem**: Julia provides C-like performance with Python/R-like ease of use. No need to rewrite bottlenecks in a low-level language - the prototype *is* the production code.

# Fitting Gaussian Random Fields with Matérn Covariance

Maximum Likelihood Estimation is the most common method to fit SWGs to data.
Typical workflows involve using a package or directly using the `optim` R function (that is used under the hood by many R packages). 
The default method is Nelder-Mead, a gradient-free optimization method.
The second choice is the BFGS quasi-Newton method where the gradients are approximated with finite differences.
Both approaches can be very slow and/or inaccurate for complex models with many parameters.
See the [optim](https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/optim){preview-link="true"} documentation for more details.

<iframe src="https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/optim" style="
                width: 100%;
            ">
</iframe>

**R as well as native Python/Matlab lack automatic differentiation (AD) capabilities.**

In Julia, the support of AD is automatic: almost any Julia code can be differentiated with Automatic Differentiation. 
Of course, Julia optimization packages also support finite differences and derivative-free methods.

## Mathematical Setup

We consider $N$ independent samples $Y^{(1)}, \ldots, Y^{(N)}$ from a spatial Gaussian Random Field (GRF) at $d$ locations, where each sample follows:

$$
Y^{(i)} \sim \mathcal{N}(0, \sigma^2 R(\nu, \rho))
$$

The correlation matrix $R$ has a **Matérn structure**:

$$
R_{jk} = \rho_{\text{Matérn}}(h_{jk}; \nu, \rho) = \frac{2^{1-\nu}}{\Gamma(\nu)} \left(\frac{h_{jk}}{\rho}\right)^\nu K_\nu\left(\frac{h_{jk}}{\rho}\right)
$$

where:

- $h_{jk}$ is the Euclidean distance between locations $j$ and $k$
- $\nu > 0$ is the **smoothness parameter** (higher $\nu$ = smoother fields)
- $\rho > 0$ is the **range parameter** (controls spatial correlation decay)
- $\sigma > 0$ is the **variance parameter** (controls overall variance)
- $K_\nu$ is the modified Bessel function of the second kind
- $\Gamma$ is the gamma function

### Log-Likelihood

For i.i.d. samples, the log-likelihood is:

$$
\ell(\nu, \rho) = -\frac{N}{2}\left[\log|R(\nu, \rho)| + d\log(2\pi)\right] - \frac{1}{2}\sum_{i=1}^{N} (Y^{(i)})^\top R(\nu, \rho)^{-1} Y^{(i)}
$$

We estimate $(\nu, \rho, \sigma)$ by minimizing $\ell$ using gradient-based optimization with **automatic differentiation**.

## Set up

Generate synthetic data from a Matérn GRF with known parameters.

```{julia}
using GaussianRandomFields
using BesselK: _gamma, adbesselkxv
using Distances
using LinearAlgebra
using Random
using Optimization, OptimizationOptimJL
import FiniteDiff, ForwardDiff
```

```{julia}
Random.seed!(1234)

# Create a d×d grid of 2D spatial locations
d = 15  # number of spatial locations per dimension (total: d²)
x_pts = range(0, stop=10, length=d)
y_pts = range(0, stop=10, length=d)

# Build location matrix (d² × 2)
locs = [(x, y) for x in x_pts, y in y_pts]
locs = hcat([[loc[1], loc[2]] for loc in locs]...)'

# Compute pairwise distances
H = pairwise(Euclidean(), locs, dims=1)
```

## Matérn Covariance Function

Define the Matérn correlation function with the `adbesselkxv` and `_gamma` functions from the [BesselK.jl](https://github.com/cgeoga/BesselK.jl) package as they are AD-compatible:

```{julia}
function cov_matérn(h, ν)
    iszero(h) && return one(h)  # Return type-stable 1
    return 2^(1 - ν) / _gamma(ν) * adbesselkxv(ν, h)
end
```

## Fixed variance $\sigma = 1$ case

### Data Generation
Generate $N$ i.i.d. samples from a Matérn GRF

```{julia}
θ_true = [2.5, 1.2] # (ρ, ν)
```

```{julia}
cov = CovarianceFunction(2, Matern(θ_true[1], θ_true[2]))
grf = GaussianRandomField(cov, GaussianRandomFields.Cholesky(), x_pts, y_pts)
```

```{julia}
N = 700
Y = [sample(grf) for _ in 1:N]

println("Generated $N samples on a $(d)×$(d) grid ($(size(H,1)) locations)")
```

### Negative Log-Likelihood

Implement the NLL using:

```{julia}
function nll(u, p)
    ρ, ν = u[1], u[2]
    Y, H = p[1], p[2]
    N = length(Y)
    d = size(H, 1)

    # Build correlation matrix R(ν, ρ) with broadcasting
    R = cov_matérn.(H / ρ, ν)

    # Cholesky decomposition (handles Σ⁻¹ and log|Σ| efficiently)
    Σ = cholesky!(Symmetric(R))

    # Negative log-likelihood:
    # ℓ = (N/2)[log|R| + d·log(2π)] + (1/2)∑ᵢ yᵢᵀ R⁻¹ yᵢ
    ℓ = N / 2 * (logdet(Σ) + d * log(2π)) +
        sum(dot(vec(y), Σ \ vec(y)) for y in Y) / 2

    return ℓ
end
```

### Parameter Estimation 

Compare optimization methods: **Automatic Differentiation** vs **Finite Differences** vs **Derivative-Free**

```{julia}
# Initial guess (true values: ρ=2.5, ν=1.2)
θ₀ = [5.2, 0.5]

# 1. Automatic Differentiation (AD) - exact gradients
optf = OptimizationFunction(nll, Optimization.AutoForwardDiff())
prob = OptimizationProblem(optf, θ₀, [Y, H], lb=[0.0, 0.0], ub=[Inf, Inf])
# 2. Finite Differences (FD) - approximate gradients
optfFD = OptimizationFunction(nll, Optimization.AutoFiniteDiff())
probFD = OptimizationProblem(optfFD, θ₀, [Y, H], lb=[0.0, 0.0], ub=[Inf, Inf])
```

I discovered that Nelder-Mead uses one evaluation of gradient for some simplex stuff see [here](https://julianlsolvers.github.io/Optim.jl/stable/algo/nelder_mead/#Nelder-Mead) and [here](https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/optim). So we use the Finite Differences version for consistency with the R version.

To limit computation time, we limit the maximum number of iterations as Finite Differences is super slow. You can increase it to reach better convergence.
```{julia}
maxiters = 400
```

Optimization Results
---------------------------
```{julia}
(solAD, timeAD) = @timed solve(prob, IPNewton(), maxiters=maxiters)
solAD.stats
```

```{julia}
(solFD, timeFD) = @timed solve(probFD, IPNewton(), maxiters=maxiters)
solFD.stats
```

```{julia}
(solNoDiff, timeNoDiff) = @timed solve(probFD, NelderMead(), maxiters=maxiters)
solNoDiff.stats
```

### Results Summary

```{julia}
results_table = [
    "True" θ_true[1] θ_true[2] "N/A";
    "AD" round(solAD.u[1], digits=3) round(solAD.u[2], digits=3) timeAD;
    "FD" round(solFD.u[1], digits=3) round(solFD.u[2], digits=3) timeFD;
    "NM" round(solNoDiff.u[1], digits=3) round(solNoDiff.u[2], digits=3) timeNoDiff
]

pretty_table(results_table,
    column_labels=["Method", "ρ", "ν", "Time (s)"], backend=:html)
```

## Free variance parameter $\sigma$

### Data Generation
```{julia}
θσ_true = [2.5, 1.2, 4.0] # (ρ, ν, σ)
```

```{julia}
covσ = CovarianceFunction(2, Matern(θσ_true[1], θσ_true[2], σ=θσ_true[3]))
grfσ = GaussianRandomField(covσ, GaussianRandomFields.Cholesky(), x_pts, y_pts)

Yσ = [sample(grfσ) for _ in 1:N]
```

### Negative Log-Likelihood
```{julia}
function nllσ(u, p)
    ρ, ν, σ = u[1], u[2], u[3]
    Y, H = p[1], p[2]
    N = length(Y)
    d = size(H, 1)

    # Build correlation matrix R(ν, ρ) with broadcasting
    R = σ^2 * cov_matérn.(H / ρ, ν)

    # Cholesky decomposition (handles Σ⁻¹ and log|Σ| efficiently)
    Σ = cholesky!(Symmetric(R))

    # Negative log-likelihood:
    # ℓ = (N/2)[log|R| + d·log(2π)] + (1/2)∑ᵢ yᵢᵀ R⁻¹ yᵢ
    ℓ = N / 2 * (logdet(Σ) + d * log(2π)) +
        sum(dot(vec(y), Σ \ vec(y)) for y in Y) / 2

    return ℓ
end
```

### Parameter Estimation

Compare optimization methods: **Automatic Differentiation** vs **Finite Differences** vs **Derivative-Free**

```{julia}
# Initial guess (true values: ρ=2.5, ν=1.2, σ = 4.0)
θσ₀ = [0.2, 0.5, 0.5]

# With automatic differentiation
optfσ = OptimizationFunction(nllσ, Optimization.AutoForwardDiff())
probσ = OptimizationProblem(optfσ, θσ₀, [Yσ, H], lb=[0.0, 0.0, 0.0], ub=[Inf, Inf, Inf])
# With finite differences
optfσFD = OptimizationFunction(nllσ, Optimization.AutoFiniteDiff())
probσFD = OptimizationProblem(optfσFD, θσ₀, [Yσ, H], lb=[0.0, 0.0, 0.0], ub=[Inf, Inf, Inf])
```

Optimization Results
---------------------------
```{julia}
(solAD, timeAD) = @timed solve(probσ, IPNewton(), maxiters=maxiters)
solAD.stats
```

Since Finite Differences is very slow, we catch potential failures to converge within the iteration budget.
```{julia}
(solFD, timeFD) = @timed try
    solve(probσFD, IPNewton(), maxiters=maxiters)
catch
    @warn "The solve failed."
    (u=[missing, missing, missing], stats=nothing)
end
solFD.stats
```

In this example, Nelder-Mead also fails to converge (producing non positive definite matrices).

```{julia}
(solNoDiff, timeNoDiff) = @timed try
    solve(probσ, NelderMead(), maxiters=maxiters)
catch
    @warn "The solve failed."
    (u=[missing, missing, missing], stats=nothing)
end
solNoDiff.stats
```

### Results Summary

```{julia}
#| code-fold: true
results_table = [
    "True" θσ_true[1] θσ_true[2] θσ_true[3] "N/A";
    "AD" round(solAD.u[1], digits=3) round(solAD.u[2], digits=3) round(solAD.u[3], digits=3) timeAD;
    "FD" round(solFD.u[1], digits=3) round(solFD.u[2], digits=3) round(solFD.u[3], digits=3) timeFD;
    "NM" round(solNoDiff.u[1], digits=3) round(solNoDiff.u[2], digits=3) round(solNoDiff.u[3], digits=3) timeNoDiff
]

pretty_table(results_table,
    column_labels=["Method", "ρ", "ν", "σ", "Time (s)"], backend=:html)
```

## Summary

To fit a two or three parameter Matérn GRF model to spatial data on a small 2D grid with 700 i.i.d. samples:

- **Finite Differences** is extremely slow and may fail to converge within a reasonable time budget. When it converges, the estimates are optimal.
- **Gradient-Free** (Nelder-Mead) is faster in this case and accurate for the two-parameter $(\rho, \nu)$ case. When adding variance to the estimation, it failed to converge. Note that it does converge when the number of samples is larger.
- **Automatic Differentiation** is both fast and converges in both settings.

This does not mean that AD is always the best choice, but it should be seriously considered when fitting complex SWG models with many parameters.

# Julia, Computer and Packages settings

```{julia}
#| code-fold: true
using InteractiveUtils
InteractiveUtils.versioninfo()

Pkg.status();
```